[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Madeline Sands",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madeline Sands",
    "section": "",
    "text": "Madeline is the Senior Category Data Manager at Scientist.com and Data Scientist. When not learning about machine learning and natural language processing, she enjoys spending time road and mountain biking, throwing pottery, and being outside."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/projects_files/mediabag/index.html",
    "href": "projects/project1/projects_files/mediabag/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe goal of their project was to test the effectiveness of a matching grant on chartiable giving. Via their large-scale natural field experiment, they found that providing a match offer increases both the charitable revenue per solicitation and the response rate to the letters. However, the larger amount of the match ratio, (i.e. $3:$1 and $2:$1), relative to a smaller match ratio, ($1:$1), had no additional impact on the revenue per solicitation nor the response rate to the letter.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nMadeline Sands\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Nomial Logit (MNL) and Conjoint Analysis\n\n\n\n\n\n\nMadeline Sands\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nMadeline Sands\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/projects_files/mediabag/index.html#introduction",
    "href": "projects/project1/projects_files/mediabag/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe goal of their project was to test the effectiveness of a matching grant on chartiable giving. Via their large-scale natural field experiment, they found that providing a match offer increases both the charitable revenue per solicitation and the response rate to the letters. However, the larger amount of the match ratio, (i.e. $3:$1 and $2:$1), relative to a smaller match ratio, ($1:$1), had no additional impact on the revenue per solicitation nor the response rate to the letter.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/projects_files/mediabag/index.html#data",
    "href": "projects/project1/projects_files/mediabag/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nimport numpy as np \nimport pandas as pd \nfrom scipy.stats import t\nfrom scipy.stats import ttest_ind\n\nkarlan_df = pd.read_stata('data/karlan_list_2007.dta')\n\n\nDescription\nBelow is a general overview of the data from Karlan et. al 2007. A sample of approximately 50,000 individuals who had given to a charitable organization since 1991 were randomized and assigned into either a “match” treatment group or a control group. The treatment group was offered a matching grant conditional on their donation, with the goal to see if the match rate increases the likelihood of a donation in a charitable setting. This dataset contains information such as if the participant was part of the treatment or control, the match ratio, and the size of the donation, in addition to further characteristics about the donor and the donation they made.\n\nkarlan_df.describe()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of their randomization mechanism, I have provided a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically different from one another.\nI have created a function called t_test_calc that calculates the t-statistic and the p-value to determine if there is a statistically signigicant difference between the variables in the treatment and test groups at a pre-determined confidence interval.\n\ndef t_test_calc(data, treatment_col, control_col, outcome_col):\n    treatment = data[data[treatment_col] == 1]\n    control = data[data[control_col] == 1]\n    mean_treatment = treatment[outcome_col].mean()\n    mean_control = control[outcome_col].mean()\n    diff_means = mean_treatment - mean_control\n    std_treatment = treatment[outcome_col].std()\n    std_control = control[outcome_col].std()\n    n_treatment = treatment[outcome_col].count()\n    n_control = control[outcome_col].count()\n    t_stat = diff_means / np.sqrt((std_treatment**2/n_treatment) + (std_control**2/n_control))\n\n    return t_stat, n_treatment, n_control\n### T-stat calculation for mrm2 Variable###\nt_stat_mrm2, n_treatment, n_control = t_test_calc(karlan_df, \"treatment\", \"control\", \"mrm2\")\ndof = n_treatment + n_control - 2\n\np_value1 = (1 - t.cdf(np.abs(t_stat_mrm2), dof)) * 2\n\nprint(f\"t-stat calculated for mrm2: {t_stat_mrm2:.4f}\")\nprint(f\"p-value calculated for mrm2: {p_value1:.4f}\")\n\n### T-stat calculation for Freq Variable###\nt_stat_freq, n_treatment, n_control = t_test_calc(karlan_df, \"treatment\", \"control\", \"freq\")\ndof_freq = n_treatment + n_control - 2\n\np_value_freq = (1 - t.cdf(np.abs(t_stat_freq), dof)) * 2\n\nprint(f\"t-stat calculated for freq: {t_stat_freq:.4f}\")\nprint(f\"p-value calculated for freq:{p_value_freq:.4f}\")\n\nt-stat calculated for mrm2: 0.1195\np-value calculated for mrm2: 0.9049\nt-stat calculated for freq: -0.1108\np-value calculated for freq:0.9117\n\n\n\n##Linear regression for mrm2 and Freq variables\nfrom sklearn.linear_model import LinearRegression\nkarlan_df.fillna({\"mrm2\": 0}, inplace=True)\n\nX = karlan_df[['treatment']]  # Feature\ny = karlan_df['mrm2']  # Target variable\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint('Coefficients for mrm2:', model.coef_)\nprint('Intercept for mrm2:', model.intercept_)\n\nCoefficients for mrm2: [0.01329623]\nIntercept for mrm2: 12.99814226643495\n\n\nAbove, I have tested the variable mrm2 & freq to see if there is a statistically significant difference between the treatment and control groups at a 95% confidence interval level. The variables mrm2 and freq represent the Number of Months since last donation and the number of prior dontations, respectively. Using the t-test-calc function, the calculated t-stat for mrm2 is 0.1195 and the p-value is 0.9049. This p-value is greater than the alpha value of 0.005, which means that we fail to reject the null hypothesis, and indicates that there is not enough evidence to conclude a statistically significant difference between the treatment and control groups when examining the number of months since their last donation, or the mrm2 variable.\nFor the freq variable, the t_stat and p_value calculated were -0.1108 and 0.9117, respectively. The calculated p-value of 0.9117 is greater than the alpha value of 0.05, and we once again fail to reject the null hypothesis. This indicates that there is not enough evidence to indicate a statistically significant difference between the treatment and control groups when comparing the donation frequency. These values are similar to those included in table 1 of Karlan et al. I believe table 1 was included in the paper to show the sample statistics of the member activity, census demographics and the state-level activity of organization."
  },
  {
    "objectID": "projects/project1/projects_files/mediabag/index.html#simulation-experiment",
    "href": "projects/project1/projects_files/mediabag/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np_control = 0.018\np_treatment = 0.022\n\nnum_draws = 10000\n\ncumulative_average = np.zeros(num_draws)\ntotal_difference = 0\n\nfor i in range(num_draws):\n    control_draw = np.random.binomial(1, p_control)\n    treatment_draw = np.random.binomial(1, p_treatment)\n    difference = treatment_draw - control_draw\n    total_difference += difference\n    cumulative_average[i] = total_difference / (i + 1)\n\n\nplt.plot(cumulative_average, color='blue')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Difference')\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average of Difference')\nplt.title('Cumulative Average of Difference in Proportions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs evident in the graph, as the number of draws increases, the cumulative average of the difference in proportions becomes more stable and approaches the true difference in means. This demonstrates that as the sample size becomes larger, the estimate of the difference in means becomes more accurate and reliable. Therefore, as the sample size increases, the random sampling variability decreases, which indicates that the larger samples provide more precise estimates of population parameters and allows statisticians to make more explicit causal claims in experimentation. (assuming that all other statistical theories are upheld)\n\n\nCentral Limit Theorem\n\np_control = 0.018\np_treatment = 0.022\n\n\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\naverage_differences = {}\n\nfor n in sample_sizes:\n    average_differences[n] = []\n    for _ in range(num_simulations):\n        control_draws = np.random.binomial(1, p_control, size=n)\n        treatment_draws = np.random.binomial(1, p_treatment, size=n)\n        average_difference = np.mean(treatment_draws) - np.mean(control_draws)\n        average_differences[n].append(average_difference)\n\nplt.figure(figsize=(12, 8))\nfor i, n in enumerate(sample_sizes):\n    plt.subplot(2, 2, i+1)\n    plt.hist(average_differences[n], bins=30, color='blue', alpha=0.7, orientation='horizontal')\n    plt.ylabel('Average Difference')\n    plt.xlabel('Frequency')\n    plt.title(f'Sample Size = {n}')\n    plt.axhline(y=0, color='red', linestyle='--')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms show the distribution of sample averages, which are the averages of a large number of samples taken from the population. According to the Central Limit Theorem, regardless of the shape of the population distribution, the distribution of sample averages tends to be normal (bell-shaped) as the sample size increases.\nIn a normal distribution, the mean (average) is located at the center of the distribution. As we increase the sample size, the distribution of sample averages becomes increasingly normal, and the mean of this distribution approaches the true population mean. Since we’re plotting the distribution of sample averages, and zero represents the mean difference (which would be the population mean difference if the samples were large enough), it’s expected that zero would be located in the center or “middle” of the distribution.\nTherefore, in the histograms representing the Central Limit Theorem, zero typically represents the “middle” of the distribution. As the sample size increases, the distribution becomes more concentrated around zero, indicating that the sample averages are more likely to be close to the true population mean difference."
  },
  {
    "objectID": "projects/project1/projects_files/mediabag/index.html#experimental-results",
    "href": "projects/project1/projects_files/mediabag/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyzed whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib as plt\nfrom matplotlib import rcParams\n\nkarlan_df_copy = karlan_df.copy()\nkarlan_grouped = karlan_df.groupby([\"treatment\", \"control\"])[\"gave\"].mean()\nax = karlan_grouped.plot(kind='bar', color = [\"purple\", \"orange\"])\n\nrcParams['font.family'] = 'serif'  # Change 'serif' to the desired font family\nrcParams['font.serif'] = ['Avenir'] \nax.set_xlabel(\"Treatment and Control\")\nax.set_ylabel(\"Proportion of People who Donated\")\n\nax.set_title(\"Proportion of People who Donated in Treatment and Control\")\n\ngroup_labels = ['Treatment', 'Control']\nax.set_xticklabels(group_labels, rotation = 0)\n\nfor i, v in enumerate(karlan_grouped):\n    ax.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n\n\n\n\n\n\n\n\nI also ran a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made, in addition to running a bivariate linear regression to demonstrate the same finding.\n\nfrom scipy.stats import ttest_ind\nimport scipy.stats as stats\nkarlan_treatment = karlan_df[karlan_df[\"treatment\"] == 1]\nkarlan_control = karlan_df[karlan_df[\"control\"] == 1]\nt_statistic, p_value = stats.ttest_ind(karlan_treatment[\"gave\"], karlan_control[\"gave\"])\n\nprint(\"T-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\n#Bivariate Linear regression on gave\nX = karlan_df[['treatment']]  # Features\ny = karlan_df['gave']  # Target variable\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint('Coefficients:', model.coef_)\nprint('Intercept:', model.intercept_)\n\nT-statistic: 3.101361000543946\np-value: 0.0019274025949016988\nCoefficients: [0.00418035]\nIntercept: 0.017858212980165173\n\n\nThe output indicates a t-statistic of approximately 3.10 and a p-value of approximately 0.002.\nIn interpreting these results, it’s important to recall that the t-statistic measures the size of the difference between the treatment and control groups relative to the variability in the data. Thus, the larger the t-statistic, the more the means of the two groups differ. In this case, a t-statistic of 3.10 suggests a substantial difference between the means of the treatment and control groups.\nThe p-value, on the other hand, assesses the probability of observing such a large difference if there were no true difference between the treatment and control groups (i.e., if the null hypothesis were true). A small p-value (in this case, 0.002) indicates that the observed difference is unlikely to have occurred by random chance alone. Interpreted in the context of the experiment, these statistical results suggest that there is a statistically significant difference in charitable giving between the treatment and control groups. In other words, the intervention or treatment likely had an effect on the behavior of individuals in the treatment group compared to those in the control group.\nTherefore, this finding may imply that the certain interventions or nudges implemented in the treatment group, the matching and challenge grant letters, were effective in encouraging charitable giving behavior. Understanding the effectiveness of these interventions sheds light on the psychological mechanisms and motivations behind charitable giving, potentially informing future strategies for promoting philanthropy and altruism.\n\nProbit Regression\nNext I ran a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable was assesment to treatment or control.\n\n#Probit Regression\nimport statsmodels.api as sm\n\nX = karlan_df[['treatment', 'control']]\ny = karlan_df['gave']\n\nmodel = sm.Probit(y, X).fit()\n\nprint(model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 17 Apr 2024   Pseudo R-squ.:               0.0009783\nTime:                        13:29:35   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment     -2.0134      0.015   -131.734      0.000      -2.043      -1.983\ncontrol       -2.1001      0.023    -90.073      0.000      -2.146      -2.054\n==============================================================================\n\n\nProbit regression is a type of regression analysis used to model binary outcomes, similar to logistic regression. In probit regression, the relationship between the predictor variables and the binary outcome is modeled using the cumulative distribution function of the standard normal distribution (also known as the probit function). The model assumes that the linear combination of predictor variables is associated with the probability of the binary outcome.\n\n\n\nDifferences between Match Rates\nNext, I assessed the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# T-test for ratio2 (2:1 match) compared to 1:1 match\nt_stat_ratio2, p_value_ratio2 = ttest_ind(karlan_df[karlan_df['ratio2'] == 1]['gave'], karlan_df[karlan_df['ratio2'] == 0]['gave'])\n\n# T-test for ratio3 (3:1 match) compared to 1:1 match\nt_stat_ratio3, p_value_ratio3 = ttest_ind(karlan_df[karlan_df['ratio3'] == 1]['gave'], karlan_df[karlan_df['ratio3'] == 0]['gave'])\n\nprint(\"T-Test Results for 2:1 match ratio:\")\nprint(f\"T-Statistic: {t_stat_ratio2}, P-Value: {p_value_ratio2}\")\n\nprint(\"\\nT-Test Results for 3:1 match ratio:\")\nprint(f\"T-Statistic: {t_stat_ratio3}, P-Value: {p_value_ratio3}\")\n\nT-Test Results for 2:1 match ratio:\nT-Statistic: 1.6725548025261596, P-Value: 0.09442121711611902\n\nT-Test Results for 3:1 match ratio:\nT-Statistic: 1.7562202653799, P-Value: 0.07905691730335489\n\n\nOverall, based on these results, we do not have enough evidence to conclude that either the 2:1 match ratio or the 3:1 match ratio has a significantly different effect on charitable donations compared to the 1:1 match ratio. These findings match those of Karlan et al. They too found that “that neither the match threshold nor the example amount had a meaningful influence on behavior.”\n\nimport pyrsm as rsm\n\nreg_ratio = rsm.model.regress(\n    data = {\"Karlan DF\": karlan_df},\n    rvar = \"gave\",\n    evar = \"ratio\"\n)\n\nreg_ratio.summary()\n\nLinear regression (OLS)\nData                 : Karlan DF\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nIn this regression, each ratio coefficient represents the effect of the corresponding ratio variable on charitable donations, holding other variables constant. Ratio[1]: The coefficient is positive (0.003), but it is not statistically significant at the 0.05 significance level (p = 0.097). Ratio[2]: The coefficient is positive (0.005) and statistically significant (p = 0.006), indicating that for each unit increase in ratio[2], charitable donations increase by 0.005 units, holding other variables constant. Ratio[3]: Similar to Ratio[2], the coefficient is positive (0.005) and statistically significant (p = 0.005), suggesting that for each unit increase in ratio[3], charitable donations increase by 0.005 units, holding other variables constant. The intercept represents the expected value of the dependent variable (charitable donations) when all explanatory variables (ratio) are zero. In this case, the intercept is statistically significant (p &lt; 0.001), indicating that when the ratio is zero, there is still a non-zero expected value of charitable donations.\nThe R-squared value (0.0) indicates that the model does not explain much of the variability in charitable donations. The F-statistic (3.665) tests the overall significance of the model. With a p-value of 0.012, the model is statistically significant, suggesting that at least one of the explanatory variables has a significant effect on charitable donations.\nOverall, the results suggest that ratio[2] and ratio[3] have a statistically significant positive effect on charitable donations, while ratio[1] does not have a statistically significant effect. However, it’s essential to consider the context of the study and potential limitations when interpreting these findings.\n\n# response rate difference between 1:1 and 2:1 match ratios\nrr_1_1 = len(karlan_df[(karlan_df['ratio'] == 1) & (karlan_df['gave'] == 1)]) / len(karlan_df[karlan_df['ratio'] == 1])\nrr_2_1 = len(karlan_df[(karlan_df['ratio'] == 2) & (karlan_df['gave'] == 1)]) / len(karlan_df[karlan_df['ratio'] == 2])\nrr_difference_1_2 = rr_2_1 - rr_1_1\n\n# response rate difference between 2:1 and 3:1 match ratios\nrr_3_1 = len(karlan_df[(karlan_df['ratio'] == 3) & (karlan_df['gave'] == 1)]) / len(karlan_df[karlan_df['ratio'] == 3])\nrr_difference_2_3 = rr_3_1 - rr_2_1\n\nprint(\"Response Rate Difference between 1:1 and 2:1 Match Ratios:\", rr_difference_1_2)\nprint(\"Response Rate Difference between 2:1 and 3:1 Match Ratios:\", rr_difference_2_3)\n\n\n### \n\ncoeff_ratio_1 = 0.003\ncoeff_ratio_2 = 0.005\ncoeff_ratio_3 = 0.005\n\n# response rate diff between 1:1 and 2:1 match ratios\nrr_diff_coef_1_2 = coeff_ratio_2 - coeff_ratio_1\n\n# response rate diff between 2:1 and 3:1 match ratios\nrr_diff_coef_2_3 = coeff_ratio_3 - coeff_ratio_2\n\nprint(\"Response Rate Difference (from Coefficients) between 1:1 and 2:1 Match Ratios:\", rr_diff_coef_1_2)\nprint(\"Response Rate Difference (from Coefficients) between 2:1 and 3:1 Match Ratios:\", rr_diff_coef_2_3)\n\nResponse Rate Difference between 1:1 and 2:1 Match Ratios: 0.0018842510217149944\nResponse Rate Difference between 2:1 and 3:1 Match Ratios: 0.00010002398025293902\nResponse Rate Difference (from Coefficients) between 1:1 and 2:1 Match Ratios: 0.002\nResponse Rate Difference (from Coefficients) between 2:1 and 3:1 Match Ratios: 0.0\n\n\nThe response rate difference between individuals in the 1:1 match ratio group and the 2:1 match ratio group is approximately 0.0019 (or 0.19%). The response rate difference (derived from the coefficients) between the 1:1 match ratio group and the 2:1 match ratio group is 0.002 (or 0.2%).\nThe response rate difference between individuals in the 2:1 match ratio group and the 3:1 match ratio group is approximately 0.0001 (or 0.01%), whereas the response rate difference (derived from the coefficients) between the 2:1 match ratio group and the 3:1 match ratio group is 0.0 (or no difference).\n1:1 vs. 2:1 Match Ratios: Both the direct data analysis and the coefficient analysis suggest that individuals in the 2:1 match ratio group have a slightly higher response rate compared to those in the 1:1 match ratio group. The difference is small but consistent across both analyses.\n2:1 vs. 3:1 Match Ratios: According to the direct data analysis, there is almost no difference in the response rates between the 2:1 match ratio group and the 3:1 match ratio group. However, the coefficient analysis indicates that the response rate difference between these groups is exactly zero.\nOverall, based on these findings, it seems that increasing the size of matched donations from 1:1 to 2:1 may lead to a slightly higher response rate, but further increasing the match ratio beyond 2:1 may not have a significant additional impact on the response rate.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyzed the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\n\ntreatment_amount = karlan_df[karlan_df['treatment'] == 1]['amount']\ncontrol_amount = karlan_df[karlan_df['treatment'] == 0]['amount']\n\n# Perform the t-test\nt_statistic, p_value = ttest_ind(treatment_amount, control_amount)\n\n# Print the results\nprint(\"T-statistic:\", t_statistic)\nprint(\"P-value:\", p_value)\n\nT-statistic: 1.8605020225753781\nP-value: 0.06282038947470683\n\n\nT-test Approach: If the p-value from the t-test is less than a chosen significance level (e.g., 0.05), it indicates that there is a statistically significant difference in donation amounts between the treatment and control groups. This suggests that treatment status, such as the use of a matching grant letter or challenge grant letter, does have a statistically significant effect on donation amounts.\nBivariate Linear Regression Approach: If the coefficient for the ‘treatment’ variable in the linear regression model is statistically significant (p-value &lt; 0.05), it indicates that treatment status has a significant effect on donation amounts. The sign of the coefficient indicates the direction of the effect (positive or negative), and the magnitude represents the size of the effect.\nBoth the t-test approach and the Bivariate Linear Regression approach provide insights into the relationship between the treatment status and donation amounts. They help in understanding whether being in the treatment group influences the donation amounts compared to the control group.\n\ndonated_df = karlan_df[karlan_df['amount'] &gt; 0]\n\n#added a constant term for the intercept\ndonated_df['intercept'] = 1\nX = donated_df[['intercept', 'treatment']]\ny = donated_df['amount']\n\nmodel = sm.OLS(y, X).fit()\n\n\nprint(model.summary())\n###\n\ntreatment_amount = donated_df[donated_df['treatment'] == 1]['amount']\ncontrol_amount = donated_df[donated_df['treatment'] == 0]['amount']\n\nt_statistic, p_value = ttest_ind(treatment_amount, control_amount)\n\nprint(\"T-statistic:\", t_statistic)\nprint(\"P-value:\", p_value)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 17 Apr 2024   Prob (F-statistic):              0.561\nTime:                        13:29:36   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-statistic: -0.5808388615237938\nP-value: 0.5614758782284279\n\n\n/var/folders/bj/t618x2614s9c896326lrdjw40000gn/T/ipykernel_7059/3304687072.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  donated_df['intercept'] = 1\n\n\nBased on these results, we can conclude that there is no significant difference in the amount donated by individuals in the treatment group compared to those in the control group. Therefore, the treatment coefficient does not have a causal interpretation regarding the effectiveness of the treatment on the amount donated. It suggests that the treatment (whatever it may be) does not significantly influence the donation amount.\n\nimport matplotlib.pyplot as plt\n\n#filtering the dataset for if a donation was made\ndonated_treatment = karlan_df[(karlan_df['amount'] &gt; 0) & (karlan_df['treatment'] == 1)]\ndonated_control = karlan_df[(karlan_df['amount'] &gt; 0) & (karlan_df['treatment'] == 0)]\n\n# sample averages for control and treatment\navg_treatment = donated_treatment['amount'].mean()\navg_control = donated_control['amount'].mean()\n\n# create histograms\nplt.figure(figsize=(10, 5))\n\n#histogram for treatment group\nplt.subplot(1, 2, 1)\nplt.hist(donated_treatment['amount'], color='blue', alpha=0.7)\nplt.axvline(avg_treatment, color='red', linestyle='dashed', linewidth=1, label='Sample Average')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.title('Treatment Group')\nplt.legend()\n\n# histogram for control group\nplt.subplot(1, 2, 2)\nplt.hist(donated_control['amount'], color='green', alpha=0.7)\nplt.axvline(avg_control, color='red', linestyle='dashed', linewidth=1, label='Sample Average')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.title('Control Group')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Madeline Sands",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA Masters of Science in Business Analytics | July 2023 - December 2015\nUniversity of California, Irvine | Irvine, CA B.S in Analytical Chemistry | Sept 2015 - March 2020"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Madeline Sands",
    "section": "Experience",
    "text": "Experience"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved compared to traditional methods. Ideal data to study such an effect and prove some type of causal relationship might include the success rate of patent applications before using Blueprinty’s software and after using it if the use of Blueprinty was randomly assigned. Unfortunately, this data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data includes each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. Blueprinty’s marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nblueprinty_df.info()\n\nprint(blueprinty_df.head())\n\ndummy_blueprinty_df = pd.get_dummies(blueprinty_df['region'], prefix='dummy_')\n\n# Concatenate the original DataFrame with the dummy variables DataFrame\nblueprinty = pd.concat([blueprinty_df, dummy_blueprinty_df], axis=1)\nprint(blueprinty.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  1500 non-null   int64  \n 1   patents     1500 non-null   int64  \n 2   region      1500 non-null   object \n 3   age         1500 non-null   float64\n 4   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(3), object(1)\nmemory usage: 58.7+ KB\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n   Unnamed: 0  patents     region   age  iscustomer  dummy__Midwest  \\\n0           1        0    Midwest  32.5           0            True   \n1         786        3  Southwest  37.5           0           False   \n2         348        4  Northwest  27.0           1           False   \n3         927        3  Northeast  24.5           0           False   \n4         830        3  Southwest  37.0           0           False   \n\n   dummy__Northeast  dummy__Northwest  dummy__South  dummy__Southwest  \n0             False             False         False             False  \n1             False             False         False              True  \n2             False              True         False             False  \n3              True             False         False             False  \n4             False             False         False              True  \n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Group {status} Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram above shows the number of patents submitted by customer status as well as includes mean bars for the two groups. In this data set, 1 indicates that a engineering firm is a blueprinty customer and a 0 indicates that they are not a blueprinty customer. Based on this histogram, Blueprinty’s customers do have a slightly higher average number of patents submitted, 4.09 patents compared to the non-blueprinty customers, 3.62 patents. However, there are a couple of important caveats to these numbers. First, the non-blueprinty customer base not only is larger than that of blueprinty’s, but the range is wider, and especially hovers around the 0-2 patent range. This means that these customers could be bringing the average down among the non-blueprinty customer base. Additionally, Blueprinty customers are not selected at random. Therefore, additional systematic differences in the age and location of the engineering firms could be further contributing to the difference in the mean number of patents submitted.\nIt may be important to account for systematic differences in the age and regional location of customers vs non-customers. Therefore, we are going to examine the distribution of Blueprinty’s customers by regions and ages.\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['region']\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=10)\n\nplt.xlabel('Number of Customers')\nplt.ylabel('Frequency')\nplt.title('Histogram of Distribution of Blueprinty Customers by Region')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nBased on region, Blueprinty has the highest number of customers located in the Northeast. However, the Northeast also has the highest number of engineering firms out of all the other regions. If we were to take the average number of Blueprinty customer’s per region, blueprinty would have approximately 10% of the engineering firms among the various regions.\n\n\n\n\ngrouped_age = blueprinty_df.groupby('iscustomer')['age']\nmeans_age = grouped_age.mean()\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped_age:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\nfor status, mean in means_age.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.5, 75, f'{status} Mean: {mean:.2f} years', rotation=90)\n\nplt.xlabel('Age of Customers (years)')\nplt.ylabel('Frequency')\nplt.title('Histogram of Customer Age')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of Blueprinty and non-Blueprinty customers by age. On average, Blueprinty’s customers are about 2.54 years younger than other customers. Additionally, there is a slight rightward skew to Blueprinty’s customers where they seem to have clients that have younger businesses, which would contribute to the younger average age of their client’s businesses. We can see that Blueprinty has been targeting younger buinesses compared to older businesses. This makes sense for Blueprinty, and could be part of their business plan. These businesses could have a greater ROI when using the Blueprinty software compared to an older business that has expertise submitting patents.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we will use a Poisson density function to model the number of patents awarded to each engineering firm over the last 5 years. Just to note, a Poisson model is a distribution of discrete counts in a pre-determined time frame, hence why we will be using it instead of another distribution model.\nWe start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, the likelihood function represents the probability of observing the data given a specific value of \\(\\lambda\\). To mathematically write down the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\), we first need to write down the density of the dependent variable for one observation. For a Poisson distribution, the density function is \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nUsing this density function, we can then estimate the likelihood by multiplying the density together N times, where the n represents the number of observations.\nNext we want to take the natural log of the density and create the joint log-likelihood by adding the N log-densities together, which can be done due to log properties. Therefore, when we take the natural log of our liklihood function, we get \\(\\ln f(Y|\\lambda) = \\sum_{i=1}^{n} (-\\lambda + x_i \\ln(\\lambda) - \\ln(\\lambda))\\) .\nNext we want to maximize our likelihood function. To find the maximum likelihood estimate of λ, we need to maximize the logarithm of the likelihood function with respect to λ. This can be done by taking the derivative of the log-likelihood function with respect to λ, setting it equal to zero, and solving for λ: \\(\\frac{d \\ln L(\\lambda)}{d\\lambda} = 0\\)\nOnce we solve this function for λ, we have the parameter value that best fits the observed data according to the Poisson distribution.\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_value, Y):\n   log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n   return log_likelihood\n\nIn python, we can create a function that takes our lambda value and Y and returns our log likelihood value.\n\n# number of patents\nY = blueprinty_df[\"patents\"]\n\n#range of lambda values\nlambda_range = np.linspace(0.1, 20, 100)\n\n# log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lambda_val, Y) for lambda_val in lambda_range]\n\nplt.plot(lambda_range, log_likelihoods)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWhen we take the first derivative of our function and set it equal to 0, we are able to solve for the value that maximizes our function. This is because the first derivative indicates a rate or slope, and we are looking for the point of our function that has a 0 slope i.e. some type of peak. From the graphical representation above, we can see that our function has a max somewhere between 2.5 and 5 for our lambda value, or approximately 3.\n\nfrom scipy.optimize import fsolve\n\ndef poisson_loglikelihood_derivative(lambda_value, Y):\n    return np.sum(-1 + Y / lambda_value)\n\n# Observed number of patents\nY = blueprinty_df[\"patents\"] # Update with your observed value\n\n# Solve for lambda using fsolve\nlambda_mle = fsolve(poisson_loglikelihood_derivative, x0=1, args=(Y,))[0]\n\nprint(f\"Maximum Likelihood Estimate (MLE) for lambda: {lambda_mle:.3f}\")\n\nMaximum Likelihood Estimate (MLE) for lambda: 3.685\n\n\nWe don’t need to calculate our first derivative by hand each time, set equal to 0 and run. Instead, we can use the function scipy.optimize and fsolve to solved our function when it is set equal to 0 and find that value that maximizes our function. In this case, this MLE for lambda is 3.685.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n    \n    #return log_likelihood\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = -np.sum(lambda_value - Y * linear_predictor + np.log(factorial(Y)))\n    #return log_likelihood\n\n#def poisson_regression_gradient(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #gradient = np.dot(X.T, lambda_value - Y)\n    #return gradient\n\n# Define the Hessian of the log-likelihood function\n#def poisson_regression_hessian(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #diag_lambda = np.diag(lambda_value)\n    #hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    #return hessian\n\n# Generate sample data (replace this with your actual data)\nX = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']].copy()\nX['constant'] = 1  # constant column\nX['age_squared'] = X['age'] ** 2\nY = blueprinty['patents']\n\n# Initial guess for beta\n#initial_beta = np.zeros(X.shape[1])\n\n# Optimize using minimize\n#result = minimize(poisson_regression_loglikelihood, initial_beta, args=(X.values, Y), jac=poisson_regression_gradient, hess=poisson_regression_hessian, method='trust-constr', options={'disp': True})\n\n# Extract MLE estimates\n#beta_mle = result.x\n\n# Compute standard errors using the inverse of the Hessian\n#covariance_matrix = np.linalg.inv(poisson_regression_hessian(beta_mle, X.values, Y))\n#standard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Print coefficients and standard errors\n#print(\"Coefficients:\")\n#for i, coef in enumerate(beta_mle):\n    ##print(f\"Beta_{i}: {coef:.4f} (SE: {standard_errors[i]:.4f})\")\n\n\nfrom scipy.optimize import minimize\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n    \n    #return log_likelihood\n\n# Define covariates matrix X \n#X = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']]\n#X['constant'] = 1  # constant column\n#X[\"age\"] = X[\"age\"].astype(int)\n# Create age squared column\n#X['age_squared'] = X['age'] ** 2\n\n\n# Convert X and Y to numpy arrays\n#X = X.values\n#Y = blueprinty['patents'].values\n\n# Initial guess for beta vector\n#initial_beta = np.ones(X.shape[1])\n\n# Optimize the log-likelihood function using scipy.optimize\n#result = minimize(poisson_regression_loglikelihood, initial_beta, args=(X, Y), method='BFGS')\n\n# MLE vector\n##mle_vector = result.x\n\n# Hessian of the Poisson model\n#hessian = result.hess_inv\n\n# Calculate standard errors of beta parameter estimates\n#se_beta = np.sqrt(np.diag(hessian))\n# Present a table of coefficients and standard errors\n#coefficients_table = pd.DataFrame({'Coefficients': mle_vector, 'Standard Errors': se_beta}, index=['constant', 'age', 'age_squared'] + list(regions[:-1]) + ['iscustomer'])\n#print(coefficients_table)\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\n#import statsmodels.api as sm\n#X = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']]\n#Y = blueprinty['patents'].values\n#model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n#print(model.summary())"
  },
  {
    "objectID": "projects/project2/index.html#blueprinty-case-study",
    "href": "projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved compared to traditional methods. Ideal data to study such an effect and prove some type of causal relationship might include the success rate of patent applications before using Blueprinty’s software and after using it if the use of Blueprinty was randomly assigned. Unfortunately, this data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data includes each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. Blueprinty’s marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nblueprinty_df.info()\n\nprint(blueprinty_df.head())\n\ndummy_blueprinty_df = pd.get_dummies(blueprinty_df['region'], prefix='dummy_')\n\n# Concatenate the original DataFrame with the dummy variables DataFrame\nblueprinty = pd.concat([blueprinty_df, dummy_blueprinty_df], axis=1)\nprint(blueprinty.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  1500 non-null   int64  \n 1   patents     1500 non-null   int64  \n 2   region      1500 non-null   object \n 3   age         1500 non-null   float64\n 4   iscustomer  1500 non-null   int64  \ndtypes: float64(1), int64(3), object(1)\nmemory usage: 58.7+ KB\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n   Unnamed: 0  patents     region   age  iscustomer  dummy__Midwest  \\\n0           1        0    Midwest  32.5           0            True   \n1         786        3  Southwest  37.5           0           False   \n2         348        4  Northwest  27.0           1           False   \n3         927        3  Northeast  24.5           0           False   \n4         830        3  Southwest  37.0           0           False   \n\n   dummy__Northeast  dummy__Northwest  dummy__South  dummy__Southwest  \n0             False             False         False             False  \n1             False             False         False              True  \n2             False              True         False             False  \n3              True             False         False             False  \n4             False             False         False              True  \n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Group {status} Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram above shows the number of patents submitted by customer status as well as includes mean bars for the two groups. In this data set, 1 indicates that a engineering firm is a blueprinty customer and a 0 indicates that they are not a blueprinty customer. Based on this histogram, Blueprinty’s customers do have a slightly higher average number of patents submitted, 4.09 patents compared to the non-blueprinty customers, 3.62 patents. However, there are a couple of important caveats to these numbers. First, the non-blueprinty customer base not only is larger than that of blueprinty’s, but the range is wider, and especially hovers around the 0-2 patent range. This means that these customers could be bringing the average down among the non-blueprinty customer base. Additionally, Blueprinty customers are not selected at random. Therefore, additional systematic differences in the age and location of the engineering firms could be further contributing to the difference in the mean number of patents submitted.\nIt may be important to account for systematic differences in the age and regional location of customers vs non-customers. Therefore, we are going to examine the distribution of Blueprinty’s customers by regions and ages.\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['region']\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=10)\n\nplt.xlabel('Number of Customers')\nplt.ylabel('Frequency')\nplt.title('Histogram of Distribution of Blueprinty Customers by Region')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nBased on region, Blueprinty has the highest number of customers located in the Northeast. However, the Northeast also has the highest number of engineering firms out of all the other regions. If we were to take the average number of Blueprinty customer’s per region, blueprinty would have approximately 10% of the engineering firms among the various regions.\n\n\n\n\ngrouped_age = blueprinty_df.groupby('iscustomer')['age']\nmeans_age = grouped_age.mean()\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped_age:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\nfor status, mean in means_age.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.5, 75, f'{status} Mean: {mean:.2f} years', rotation=90)\n\nplt.xlabel('Age of Customers (years)')\nplt.ylabel('Frequency')\nplt.title('Histogram of Customer Age')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of Blueprinty and non-Blueprinty customers by age. On average, Blueprinty’s customers are about 2.54 years younger than other customers. Additionally, there is a slight rightward skew to Blueprinty’s customers where they seem to have clients that have younger businesses, which would contribute to the younger average age of their client’s businesses. We can see that Blueprinty has been targeting younger buinesses compared to older businesses. This makes sense for Blueprinty, and could be part of their business plan. These businesses could have a greater ROI when using the Blueprinty software compared to an older business that has expertise submitting patents.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we will use a Poisson density function to model the number of patents awarded to each engineering firm over the last 5 years. Just to note, a Poisson model is a distribution of discrete counts in a pre-determined time frame, hence why we will be using it instead of another distribution model.\nWe start by estimating a simple Poisson model via Maximum Likelihood.\nFirst, the likelihood function represents the probability of observing the data given a specific value of \\(\\lambda\\). To mathematically write down the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\), we first need to write down the density of the dependent variable for one observation. For a Poisson distribution, the density function is \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nUsing this density function, we can then estimate the likelihood by multiplying the density together N times, where the n represents the number of observations.\nNext we want to take the natural log of the density and create the joint log-likelihood by adding the N log-densities together, which can be done due to log properties. Therefore, when we take the natural log of our liklihood function, we get \\(\\ln f(Y|\\lambda) = \\sum_{i=1}^{n} (-\\lambda + x_i \\ln(\\lambda) - \\ln(\\lambda))\\) .\nNext we want to maximize our likelihood function. To find the maximum likelihood estimate of λ, we need to maximize the logarithm of the likelihood function with respect to λ. This can be done by taking the derivative of the log-likelihood function with respect to λ, setting it equal to zero, and solving for λ: \\(\\frac{d \\ln L(\\lambda)}{d\\lambda} = 0\\)\nOnce we solve this function for λ, we have the parameter value that best fits the observed data according to the Poisson distribution.\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_value, Y):\n   log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n   return log_likelihood\n\nIn python, we can create a function that takes our lambda value and Y and returns our log likelihood value.\n\n# number of patents\nY = blueprinty_df[\"patents\"]\n\n#range of lambda values\nlambda_range = np.linspace(0.1, 20, 100)\n\n# log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lambda_val, Y) for lambda_val in lambda_range]\n\nplt.plot(lambda_range, log_likelihoods)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWhen we take the first derivative of our function and set it equal to 0, we are able to solve for the value that maximizes our function. This is because the first derivative indicates a rate or slope, and we are looking for the point of our function that has a 0 slope i.e. some type of peak. From the graphical representation above, we can see that our function has a max somewhere between 2.5 and 5 for our lambda value, or approximately 3.\n\nfrom scipy.optimize import fsolve\n\ndef poisson_loglikelihood_derivative(lambda_value, Y):\n    return np.sum(-1 + Y / lambda_value)\n\n# Observed number of patents\nY = blueprinty_df[\"patents\"] # Update with your observed value\n\n# Solve for lambda using fsolve\nlambda_mle = fsolve(poisson_loglikelihood_derivative, x0=1, args=(Y,))[0]\n\nprint(f\"Maximum Likelihood Estimate (MLE) for lambda: {lambda_mle:.3f}\")\n\nMaximum Likelihood Estimate (MLE) for lambda: 3.685\n\n\nWe don’t need to calculate our first derivative by hand each time, set equal to 0 and run. Instead, we can use the function scipy.optimize and fsolve to solved our function when it is set equal to 0 and find that value that maximizes our function. In this case, this MLE for lambda is 3.685.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n    \n    #return log_likelihood\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = -np.sum(lambda_value - Y * linear_predictor + np.log(factorial(Y)))\n    #return log_likelihood\n\n#def poisson_regression_gradient(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #gradient = np.dot(X.T, lambda_value - Y)\n    #return gradient\n\n# Define the Hessian of the log-likelihood function\n#def poisson_regression_hessian(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #diag_lambda = np.diag(lambda_value)\n    #hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    #return hessian\n\n# Generate sample data (replace this with your actual data)\nX = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']].copy()\nX['constant'] = 1  # constant column\nX['age_squared'] = X['age'] ** 2\nY = blueprinty['patents']\n\n# Initial guess for beta\n#initial_beta = np.zeros(X.shape[1])\n\n# Optimize using minimize\n#result = minimize(poisson_regression_loglikelihood, initial_beta, args=(X.values, Y), jac=poisson_regression_gradient, hess=poisson_regression_hessian, method='trust-constr', options={'disp': True})\n\n# Extract MLE estimates\n#beta_mle = result.x\n\n# Compute standard errors using the inverse of the Hessian\n#covariance_matrix = np.linalg.inv(poisson_regression_hessian(beta_mle, X.values, Y))\n#standard_errors = np.sqrt(np.diag(covariance_matrix))\n\n# Print coefficients and standard errors\n#print(\"Coefficients:\")\n#for i, coef in enumerate(beta_mle):\n    ##print(f\"Beta_{i}: {coef:.4f} (SE: {standard_errors[i]:.4f})\")\n\n\nfrom scipy.optimize import minimize\n\n#def poisson_regression_loglikelihood(beta, X, Y):\n    #linear_predictor = np.dot(X, beta)\n    #lambda_value = np.exp(linear_predictor)\n    #log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n    \n    #return log_likelihood\n\n# Define covariates matrix X \n#X = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']]\n#X['constant'] = 1  # constant column\n#X[\"age\"] = X[\"age\"].astype(int)\n# Create age squared column\n#X['age_squared'] = X['age'] ** 2\n\n\n# Convert X and Y to numpy arrays\n#X = X.values\n#Y = blueprinty['patents'].values\n\n# Initial guess for beta vector\n#initial_beta = np.ones(X.shape[1])\n\n# Optimize the log-likelihood function using scipy.optimize\n#result = minimize(poisson_regression_loglikelihood, initial_beta, args=(X, Y), method='BFGS')\n\n# MLE vector\n##mle_vector = result.x\n\n# Hessian of the Poisson model\n#hessian = result.hess_inv\n\n# Calculate standard errors of beta parameter estimates\n#se_beta = np.sqrt(np.diag(hessian))\n# Present a table of coefficients and standard errors\n#coefficients_table = pd.DataFrame({'Coefficients': mle_vector, 'Standard Errors': se_beta}, index=['constant', 'age', 'age_squared'] + list(regions[:-1]) + ['iscustomer'])\n#print(coefficients_table)\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\n#import statsmodels.api as sm\n#X = blueprinty[['age', 'dummy__Midwest', 'dummy__Northeast', 'dummy__Northwest', 'dummy__South','dummy__Southwest', 'iscustomer']]\n#Y = blueprinty['patents'].values\n#model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n#print(model.summary())"
  },
  {
    "objectID": "projects/project2/index.html#airbnb-case-study",
    "href": "projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "projects/project2/index.html#introduction-2",
    "href": "projects/project2/index.html#introduction-2",
    "title": "Poisson Regression Examples",
    "section": "Introduction",
    "text": "Introduction\nHere, we have been provided with 40,000 Airbnb listings from New York City. We are assuming that the number os reviews are a good proxy for the number of bookings on the Airbnb platform. Our goal is going to attempt to find the number of bookings an airbnb listing has via a function of the number of reviews the airbnb listing has received.\n\nData\n\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nairbnb = pd.read_csv(\"data/airbnb.csv\")\nairbnb.head()\n\nprint(airbnb.isnull().sum())\nairbnb = airbnb[[\"id\", \"days\", \"last_scraped\", \"host_since\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\", \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"]]\n\nairbnb.info()\n\nairbnb.dropna(inplace = True)\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   id                         40628 non-null  int64  \n 1   days                       40628 non-null  int64  \n 2   last_scraped               40628 non-null  object \n 3   host_since                 40593 non-null  object \n 4   room_type                  40628 non-null  object \n 5   bathrooms                  40468 non-null  float64\n 6   bedrooms                   40552 non-null  float64\n 7   price                      40628 non-null  int64  \n 8   number_of_reviews          40628 non-null  int64  \n 9   review_scores_cleanliness  30433 non-null  float64\n 10  review_scores_location     30374 non-null  float64\n 11  review_scores_value        30372 non-null  float64\n 12  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(4), object(4)\nmemory usage: 4.0+ MB\n\n\nWe will first conduct some exploratory data analysis to get a feel for the data. From airbnb.info() we see that we have 13 variables in our dataframe with the max number of rows being 40628. There do be some missing values, such as in bedrooms, host_since, review_scores_cleanliness, review_scores_location, and review_scores_value.\nFirst, we want to examine the distribution of room_type by the number of reviews number_of_reviews.\n\nairbnb_room = airbnb[['room_type']]\nplt.hist(airbnb_room, bins=20, alpha = 0.5)\nplt.xlabel('Airbnb Room Type and Number of Reviews')\nplt.ylabel('Frequency')\nplt.title('Histogram of Room Type and Number of reviews')\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, Airbnb tends to have either private rooms and entire homes or apts. There is not much information on shared rooms on the Airbnb platform.\n\nairbnb_price = airbnb[['price']]\nx_range = (0, 2200)\nplt.hist(airbnb_price, range =x_range, bins=100, alpha = 0.5)\n\nplt.xlabel('Price of Airbnb Listing')\nplt.ylabel('Frequency')\nplt.title('Histogram of Price')\nplt.show()\n\n\n\n\n\n\n\n\nThe price of airbnb listing’s range from close to 0 all the way up to 2000 per night. The most common price seems to be around $100/night. Additionally, because we are missing some variables from the price column, we want to potentially fill in the missing information to run a logistic regression from this data.\n\nsns.pairplot(airbnb[['bathrooms', 'bedrooms', 'price', 'number_of_reviews']])\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pyrsm as rsm\n\nmapping = {\"t\": 1, \"f\": 0}\nairbnb[\"instant_bookable\"] = airbnb[\"instant_bookable\"].map(mapping)\n\n# want to build a Poisson regression model\nX = airbnb[['bathrooms', 'bedrooms', 'price', \"review_scores_cleanliness\", \"instant_bookable\", \"review_scores_value\", \"review_scores_location\"]]\ny = airbnb['number_of_reviews']\n\n# Add constant for intercept\nX = sm.add_constant(X)\n\n# Fit Poisson regression model\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# Print model summary\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30140\nModel:                            GLM   Df Residuals:                    30132\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2905e+05\nDate:                Thu, 09 May 2024   Deviance:                   9.3672e+05\nTime:                        18:59:35   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5518\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.5414      0.016    224.237      0.000       3.510       3.572\nbathrooms                    -0.1286      0.004    -34.537      0.000      -0.136      -0.121\nbedrooms                      0.0785      0.002     39.944      0.000       0.075       0.082\nprice                      1.059e-05   7.43e-06      1.425      0.154   -3.97e-06    2.51e-05\nreview_scores_cleanliness     0.1136      0.001     76.229      0.000       0.111       0.117\ninstant_bookable              0.3321      0.003    115.247      0.000       0.326       0.338\nreview_scores_value          -0.0916      0.002    -51.094      0.000      -0.095      -0.088\nreview_scores_location       -0.0754      0.002    -47.098      0.000      -0.079      -0.072\n=============================================================================================\n\n\nThe output provided is from a Poisson regression model that aims to predict the number of reviews as a proxy for the number of bookings based on various predictor variables. Here’s how we can interpret the model coefficients in the context of the given statement:\n\n\nIntercept (const): The intercept term represents the expected number of reviews when all predictor variables are zero. In this model, the intercept is 3.5414. This means that when all other predictor variables are zero, we would expect approximately 34.41 reviews.\n\n\nbathrooms: The coefficient for the bathrooms variable is -0.1286. This suggests that for each additional bathroom, we would expect the number of reviews to decrease by approximately 0.1286, holding other variables constant. This might imply that listings with more bathrooms tend to receive fewer reviews, possibly due to higher pricing or reduced occupancy.\n\n\nbedrooms: The coefficient for the bedrooms variable is 0.0785. This indicates that for each additional bedroom, we would expect the number of reviews to increase by approximately 0.0785, holding other variables constant. This suggests that listings with more bedrooms tend to receive more reviews, possibly because they accommodate larger groups or families.\n\n\nprice: The coefficient for the price variable is 1.059e-05, but its p-value is 0.154, which is greater than the typical significance level of 0.05. This suggests that the effect of price on the number of reviews is not statistically significant at the 95% confidence level. In other words, changes in price do not have a significant impact on the number of reviews in this model.\n\n\nreview_scores_cleanliness: The coefficient for the review_scores_cleanliness variable is 0.1136. This suggests that for each unit increase in cleanliness score, we would expect the number of reviews to increase by approximately 0.1136, holding other variables constant. This implies that listings with higher cleanliness scores tend to receive more reviews.\n\n\ninstant_bookable: The coefficient for the instant_bookable variable is 0.3321. This indicates that listings that are instantly bookable tend to receive more reviews compared to those that are not, with an increase of approximately 0.3321 in the expected number of reviews.\n\n\nreview_scores_value: The coefficient for the review_scores_value variable is -0.0916. This suggests that for each unit increase in value score, we would expect the number of reviews to decrease by approximately 0.0916, holding other variables constant. This might indicate that listings perceived as offering better value tend to receive fewer reviews.\n\n\nreview_scores_location: The coefficient for the review_scores_location variable is -0.0754. This indicates that for each unit increase in location score, we would expect the number of reviews to decrease by approximately 0.0754, holding other variables constant. This could imply that listings in better locations receive fewer reviews, possibly because they are more expensive or have fewer available dates due to high demand.\n\n\nOverall, this model suggests that various factors such as the number of bathrooms, bedrooms, cleanliness scores, instant bookability, and value perception significantly influence the number of reviews received, while the effect of price and location scores is not statistically significant at the chosen significance level."
  },
  {
    "objectID": "projects/project2/Interactive-1.html",
    "href": "projects/project2/Interactive-1.html",
    "title": "Madeline Sands",
    "section": "",
    "text": "Connected to Python 3.11.6\n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nNameError: name 'pd' is not defined\n\n\n\nimport pandas as pd \nimport numpy as np \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\n\nimport pandas as pd \nimport numpy as np \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nprint(blueprinty_df.head())\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib as plt \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nprint(blueprinty_df.head())\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\n\nblueprinty_df.hist(figsize=(20,20))\n\narray([[&lt;Axes: title={'center': 'Unnamed: 0'}&gt;,\n        &lt;Axes: title={'center': 'patents'}&gt;],\n       [&lt;Axes: title={'center': 'age'}&gt;,\n        &lt;Axes: title={'center': 'iscustomer'}&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nTypeError: 'module' object is not callable\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\n\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nAttributeError: module 'matplotlib' has no attribute 'hist'\n\n\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nblueprinty_df = pd.read_csv(\"data/blueprinty.csv\")\n\nprint(blueprinty_df.head())\n\n   Unnamed: 0  patents     region   age  iscustomer\n0           1        0    Midwest  32.5           0\n1         786        3  Southwest  37.5           0\n2         348        4  Northwest  27.0           1\n3         927        3  Northeast  24.5           0\n4         830        3  Southwest  37.0           0\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'{status} Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['patents']\nmeans = grouped.mean()\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n# Overlay mean lines\nfor status, mean in means.items():\n    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n    plt.text(mean + 0.1, 50, f'Group {status} Mean: {mean:.2f}', rotation=90)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ngrouped = blueprinty_df.groupby('iscustomer')['region']\n\n# Plot histograms for each group\nplt.figure(figsize=(10, 6))\nfor status, group_data in grouped:\n    plt.hist(group_data, alpha=0.5, label=status, bins=20)\n\n\nplt.xlabel('Number of Customers')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda, Y):\n   log_likelihood = np.sum(-lambda + Y * np.log(lambda) - np.log(factorial(Y)))\n    return log_likelihood\n\nSyntaxError: invalid syntax (&lt;ipython-input-13-ae46d2fd1004&gt;, line 3)\n\n\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda, Y):\n   log_likelihood = np.sum(-lambda + Y * np.log(lambda) - np.log(factorial(Y))\n   return log_likelihood\n\nSyntaxError: invalid syntax (&lt;ipython-input-14-fcc0dbaf773f&gt;, line 3)\n\n\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda, Y):\n   log_likelihood = np.sum(-lambda + Y * np.log(lambda) - np.log(factorial(Y)))\n   return log_likelihood\n\nSyntaxError: invalid syntax (&lt;ipython-input-15-18d9d4e4a582&gt;, line 3)\n\n\n\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_value, Y):\n   log_likelihood = np.sum(-lambda_value + Y * np.log(lambda_value) - np.log(factorial(Y)))\n   return log_likelihood\n\n\n# Observed number of patents\nY = blueprinty_df[\"patents\"]  # Update with your observed value\n\n# Define a range of lambda values\nlambda_range = np.linspace(0.1, 20, 100)  # Adjust the range as needed\n\n# Calculate log-likelihood for each lambda\nlog_likelihoods = [poisson_loglikelihood(lambda_val, Y) for lambda_val in lambda_range]\n\n# Plot lambda values against log-likelihood\nplt.plot(lambda_range, log_likelihoods)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.optimize import fsolve\n\ndef poisson_loglikelihood_derivative(lambda_value, Y):\n    return np.sum(-1 + Y / lambda_value)\n\n# Observed number of patents\nY = blueprinty_df[\"patents\"] # Update with your observed value\n\n# Solve for lambda using fsolve\nlambda_mle = fsolve(poisson_loglikelihood_derivative, x0=1, args=(Y,))[0]\n\nprint(\"Maximum Likelihood Estimate (MLE) for lambda:\", lambda_mle)\n\nMaximum Likelihood Estimate (MLE) for lambda: 3.6846666666666663"
  },
  {
    "objectID": "projects/project3/index.html",
    "href": "projects/project3/index.html",
    "title": "Multi-Nomial Logit (MNL) and Conjoint Analysis",
    "section": "",
    "text": "This assignment uses a Multi-nomial Logit (MNL) model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/index.html#estimating-yogurt-preferences",
    "href": "projects/project3/index.html#estimating-yogurt-preferences",
    "title": "Multi-Nomial Logit(MNL) and Conjoint Analysis",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nA multi-Noimal Logit model is a statistical model used for modeling choices among multiple alternatives. In an MNL model, the dependent variable represents a categorical outcome with more than two categories. In a MNL model, we assume that the probability of choosing wach alternative is determined by a linear combination of explanatory variables.\nOur MNL model will estimate parameters for each explanatory variable which represent the effect of that variable on the odds of choosin one alternative over the others. These parameters are typically estimated using maximum likelihood estimation.\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose we also have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, maybe show the first few rows, and describe the data a bit.\n\nimport pandas as pd \nimport numpy as np \n\nyogurt = pd.read_csv('data/yogurt_data.csv')\nyogurt.head(10)\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n5\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.092\n0.050\n0.079\n\n\n6\n7\n0\n1\n0\n0\n0\n0\n0\n0\n0.103\n0.081\n0.049\n0.079\n\n\n7\n8\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.086\n0.054\n0.079\n\n\n8\n9\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n9\n10\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for features and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\ntodo: reshape and prep the data\n\nfor i in range(1, 4):\n    yogurt[f'product_{i}'] = (yogurt[f'y{i}'] == 1).astype(int)\n\n# Now, reshape the data from wide to long format\nyogurt_long = pd.melt(yogurt, id_vars=['id', 'p1', 'p2', 'p3', 'p4'], \n                  value_vars=['y1', 'y2', 'y3', 'y4'], \n                  var_name='product', value_name='chosen')\nyogurt_long.head()\n\n\n\n\n\n\n\n\n\nid\np1\np2\np3\np4\nproduct\nchosen\n\n\n\n\n0\n1\n0.108\n0.081\n0.061\n0.079\ny1\n0\n\n\n1\n2\n0.108\n0.098\n0.064\n0.075\ny1\n0\n\n\n2\n3\n0.108\n0.098\n0.061\n0.086\ny1\n0\n\n\n3\n4\n0.108\n0.098\n0.061\n0.086\ny1\n0\n\n\n4\n5\n0.125\n0.098\n0.049\n0.079\ny1\n0\n\n\n\n\n\n\n\n\n\n\nEstimation\ntodo: Code up the log-likelihood function.\n\n'''\ndef log_likelihood(params, data):\n    \"\"\"\n    Compute the log-likelihood of the multinomial logit model given the parameters and data.\n    \n    Parameters:\n        params (array-like): Parameters to be estimated.\n        data (DataFrame): DataFrame containing the data.\n        \n    Returns:\n        float: Log-likelihood value.\n    \"\"\"\n    # Extracting relevant data from the DataFrame\n    id_vals = data['id'].values\n    chosen_vals = data['chosen'].values\n    features = data[['f1', 'f2', 'f3', 'f4']].values\n    prices = data[['p1', 'p2', 'p3', 'p4']].values\n    \n    # Extracting parameters\n    beta = params[:-1]  # Coefficients for product features and prices\n    gamma = params[-1]   # Scale parameter for the extreme value error term\n    \n    # Calculate utility for each alternative for each observation\n    utilities = np.dot(features, beta) + np.dot(prices, beta[-4:])\n    \n    # Calculate the log-sum-exp term for each observation\n    log_sum_exp_term = np.log(np.sum(np.exp(utilities), axis=1))\n    \n    # Calculate log-likelihood for each observation\n    individual_log_likelihoods = utilities[np.arange(len(id_vals)), chosen_vals] - log_sum_exp_term\n    \n    # Calculate the joint log-likelihood\n    joint_log_likelihood = np.sum(individual_log_likelihoods)\n    \n    return joint_log_likelihood'''\n\n'\\ndef log_likelihood(params, data):\\n    \"\"\"\\n    Compute the log-likelihood of the multinomial logit model given the parameters and data.\\n    \\n    Parameters:\\n        params (array-like): Parameters to be estimated.\\n        data (DataFrame): DataFrame containing the data.\\n        \\n    Returns:\\n        float: Log-likelihood value.\\n    \"\"\"\\n    # Extracting relevant data from the DataFrame\\n    id_vals = data[\\'id\\'].values\\n    chosen_vals = data[\\'chosen\\'].values\\n    features = data[[\\'f1\\', \\'f2\\', \\'f3\\', \\'f4\\']].values\\n    prices = data[[\\'p1\\', \\'p2\\', \\'p3\\', \\'p4\\']].values\\n    \\n    # Extracting parameters\\n    beta = params[:-1]  # Coefficients for product features and prices\\n    gamma = params[-1]   # Scale parameter for the extreme value error term\\n    \\n    # Calculate utility for each alternative for each observation\\n    utilities = np.dot(features, beta) + np.dot(prices, beta[-4:])\\n    \\n    # Calculate the log-sum-exp term for each observation\\n    log_sum_exp_term = np.log(np.sum(np.exp(utilities), axis=1))\\n    \\n    # Calculate log-likelihood for each observation\\n    individual_log_likelihoods = utilities[np.arange(len(id_vals)), chosen_vals] - log_sum_exp_term\\n    \\n    # Calculate the joint log-likelihood\\n    joint_log_likelihood = np.sum(individual_log_likelihoods)\\n    \\n    return joint_log_likelihood'\n\n\ntodo: Use optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\n'''\nfrom scipy.optimize import minimize\n#create a function that returns the negative log liklihood of our function to minimize it.\ndef neg_log_likelihood(params, data):\n    return -log_likelihood(params, data)\n\ninitial_guess = np.zeros(6)  # 6 parameters: beta1, beta2, beta3, beta_f, beta_p, gamma\n\n# Bounds for parameters: beta1, beta2, beta3, beta_f, beta_p, gamma\nbounds = [(-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (0, np.inf)]\n\n# Minimize the negative log-likelihood function\nresult = minimize(neg_log_likelihood, initial_guess, args=(yogurt_long,), bounds=bounds)\n\n# Extract the MLEs\nmle_params = result.x\nmle_params'''\n\n'\\nfrom scipy.optimize import minimize\\n#create a function that returns the negative log liklihood of our function to minimize it.\\ndef neg_log_likelihood(params, data):\\n    return -log_likelihood(params, data)\\n\\ninitial_guess = np.zeros(6)  # 6 parameters: beta1, beta2, beta3, beta_f, beta_p, gamma\\n\\n# Bounds for parameters: beta1, beta2, beta3, beta_f, beta_p, gamma\\nbounds = [(-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (-np.inf, np.inf), (0, np.inf)]\\n\\n# Minimize the negative log-likelihood function\\nresult = minimize(neg_log_likelihood, initial_guess, args=(yogurt_long,), bounds=bounds)\\n\\n# Extract the MLEs\\nmle_params = result.x\\nmle_params'\n\n\n\n\nDiscussion\nWe learn…\ntodo: interpret the 3 product intercepts (which yogurt is most preferred?).\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\ntodo: calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities. Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?"
  },
  {
    "objectID": "projects/project3/index.html#estimating-minivan-preferences",
    "href": "projects/project3/index.html#estimating-minivan-preferences",
    "title": "Multi-Nomial Logit(MNL) and Conjoint Analysis",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\ntodo: download the dataset from here: http://goo.gl/5xQObB\ntodo: describe the data a bit. How many respondents took the conjoint survey? How many choice tasks did each respondent complete? How many alternatives were presented on each choice task? For each alternative.\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\ntodo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\n\nResults\ntodo: Interpret the coefficients. Which features are more preferred?\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nhint: this example is taken from the “R 4 Marketing Research” book by Chapman and Feit. I believe the same example is present in the companion book titled “Python 4 Marketing Research”. I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to “the answers,” you may consult the Chapman and Feit books."
  },
  {
    "objectID": "projects/project3/index.html#estimating-yogurt-preferences-using-a-multi-nomial-logit-model",
    "href": "projects/project3/index.html#estimating-yogurt-preferences-using-a-multi-nomial-logit-model",
    "title": "Multi-Nomial Logit (MNL) and Conjoint Analysis",
    "section": "1. Estimating Yogurt Preferences using a Multi-nomial logit model",
    "text": "1. Estimating Yogurt Preferences using a Multi-nomial logit model\n\nBackground on the Likelihood for the Multi-nomial logit Model\nA multi-nomimal logit (MNL) model is an extension of the traditional binary logit model and used when the dependent variable has more than two categories or levels. In a MNL model, the probabilities of each category are modeled simultaneously using a set of independent variables and the probabilities sum up to 1 across all categories.\nOur MNL model will estimate parameters for each explanatory variable which represent the effect of that variable on the odds of choosing one alternative over the others. These parameters are typically estimated using maximum likelihood estimation.\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it.\nSuppose we also have a vector of data on each product that is represented by \\(x_j\\) (eg, size, price, etc.).\nWe can then model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(x_j\\) is the vector of product data and \\(\\epsilon_{ij}\\) is a random error term.\nBecause we have chosen the specific type of randomness (The choice of the i.i.d. extreme value error term), it becomes possible to derive a simple formula to predict the likelihood of a consumer choosing a particular product from the set of products listed:\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability or likelihood that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nTherefore, to figure out how likely it is that a consumer made their specific choice of product, you can multiply the probabilities of choosing each product together, but only the probability of the actual chosen product affects the final result because of the indicator variable. r \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable(\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nJust to note, the \\[\\delta_{ij}\\] indicator variable acts as a switch that is “on” (equal to 1) when the specific condition (consumer ii choosing product jj) is met, and “off” (equal to 0) when it is not. In the context of the likelihood function described, the indicator variable helps in the following way:\n\nWhen \\(\\delta_{ij}\\) = 1, the probability of choosing product jj is included in the calculation.\nWhen \\(\\delta_{ij}\\) = 0, the probability of choosing product jj is effectively ignored (since raising any number to the power of 0 gives 1, which does not affect the product).\n\nNotice that if an individual consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\), then the individual likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nTo find the overall probability of all consumers making their choices, we will find the joint likelihood by simply multiplying the individual probabilities for each consumer together:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nFinally, we can take the logarithm of the joint likelihood. This changes the multiplication of probabilities into a sum of logarithms, which makes it much easier math to hande.\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, which for consumer 2, 3, 4, and 5 was priced at 0.098/oz. Customer 6 also purchased yogurt 2, but when it was priced at 0.092/oz and consumber 7 purchased it at 0.081/oz.\n\nimport pandas as pd \nimport numpy as np \n\nyogurt = pd.read_csv('data/yogurt_data.csv')\nyogurt.head(15)\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n5\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.092\n0.050\n0.079\n\n\n6\n7\n0\n1\n0\n0\n0\n0\n0\n0\n0.103\n0.081\n0.049\n0.079\n\n\n7\n8\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.086\n0.054\n0.079\n\n\n8\n9\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n9\n10\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n10\n11\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n11\n12\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n12\n13\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.050\n0.086\n\n\n13\n14\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.050\n0.086\n\n\n14\n15\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.050\n0.079\n\n\n\n\n\n\n\n\nThis data gives us an overview of which customer bought what yogurt brand at a specific price and indicates if the product was featured at the time. Let the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbf{1}(\\text{Yogurt 1}), \\mathbf{1}(\\text{Yogurt 2}), \\mathbf{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nTo start to calculate our maximum liklihood for our MNL model, we first want to reorganize the data from our wide to long shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for features and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long. By reorganizing the data this way, we can effectively analyze the likelihood of different choices using the MNL model.\n\nmelted_df = pd.melt(yogurt, id_vars=['id'], value_vars=['y1', 'y2', 'y3'], var_name='feature', value_name='yogurt_chosen')\n\nmelted_featured = pd.melt(yogurt, id_vars= ['id'], value_vars = ['f1', \"f2\", 'f3'], var_name='featured', value_name='X_featured')\n\nmelted_price = pd.melt(yogurt, id_vars= ['id'], value_vars = ['p1', \"p2\", 'p3'], var_name='value_p', value_name='X_price')\n\nyogurt_melt = pd.merge(melted_df, melted_featured, on = \"id\")\nyogurt_melt1 = pd.merge(yogurt_melt, melted_price, on = 'id')\n\nprint(yogurt_melt1.head())\n\n   id feature  yogurt_chosen featured  X_featured value_p  X_price\n0   1      y1              0       f1           0      p1    0.108\n1   1      y1              0       f1           0      p2    0.081\n2   1      y1              0       f1           0      p3    0.061\n3   1      y1              0       f2           0      p1    0.108\n4   1      y1              0       f2           0      p2    0.081\n\n\n\n#creating our binary variables based on if Y1, Y2, or Y3 is in the row\nyogurt_melt1[\"X_yogurt1\"] = yogurt_melt1[\"feature\"].apply(lambda x: 1 if x =='y1' else 0)\nyogurt_melt1[\"X_yogurt2\"] = yogurt_melt1[\"feature\"].apply(lambda x: 1 if x =='y2' else 0)\nyogurt_melt1[\"X_yogurt3\"] = yogurt_melt1[\"feature\"].apply(lambda x: 1 if x =='y3' else 0)\n\n\n# Drop the unnecessary yogurt columns\nyogurt_melt1 = yogurt_melt1.drop(columns=['feature', 'featured', 'value_p'])\nprint(yogurt_melt1.head())\n\n   id  yogurt_chosen  X_featured  X_price  X_yogurt1  X_yogurt2  X_yogurt3\n0   1              0           0    0.108          1          0          0\n1   1              0           0    0.081          1          0          0\n2   1              0           0    0.061          1          0          0\n3   1              0           0    0.108          1          0          0\n4   1              0           0    0.081          1          0          0\n\n\n\nyogurt_melt1 = yogurt_melt1[[\"id\", \"X_yogurt1\", \"X_yogurt2\", \"X_yogurt3\", \"X_featured\",\"X_price\", \"yogurt_chosen\"]]\n\nNow we have a dataframe that will enable us to effectively analyze the likelihood of different choices via our MNL model. To being this process, we need to define our log-likelihood function. Our log_likelihood function has 3 inputs: - params - the array containing the parameter values to be optimized - X - Design matrix with predictors - y - Dependent variable representing the choice made by each observation\nOur function will return the log-likelihood of the MNL Model.\n\nX = yogurt_melt1[['X_yogurt1', 'X_yogurt2', 'X_yogurt3','X_price','X_featured']].values\ny= yogurt_melt1['yogurt_chosen'].values\n\ndef log_likelihood(params, data):\n    beta1, beta2, beta3, beta_f, beta_p = params\n    \n    # Extract data\n    id = data['id']\n    value = data['yogurt_chosen']\n    yogurt_1 = data['X_yogurt1']\n    yogurt_2 = data['X_yogurt2']\n    yogurt_3 = data['X_yogurt3']\n    X_price = data['X_price']\n    X_featured = data['X_featured']\n    \n    # Calculate probability of buying\n    p_buy = np.exp(beta1 * yogurt_1 + beta2 * yogurt_2 + beta3 * yogurt_3 + beta_f * X_featured + beta_p * X_price) / (1 + np.exp(beta1 * yogurt_1 + beta2 * yogurt_2 + beta3 * yogurt_3 + beta_f * X_featured + beta_p * X_price))\n    \n    # Calculate log-likelihood for each observation\n    log_likelihoods = value * np.log(p_buy) + (1 - value) * np.log(1 - p_buy)\n    \n    # Sum up log-likelihoods to get the joint log likelihood\n    joint_log_likelihood_value = np.sum(log_likelihoods)\n    \n    return joint_log_likelihood_value\n\ntodo: Use optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\nfrom scipy.optimize import minimize\nfrom scipy import optimize\n\n# Define log-likelihood function\n\ndef neg_log_likelihood(params, data):\n    beta1, beta2, beta3, beta_f, beta_p = params\n    \n    # Extract data\n    id = data['id']\n    value = data['yogurt_chosen']\n    yogurt_1 = data['X_yogurt1']\n    yogurt_2 = data['X_yogurt2']\n    yogurt_3 = data['X_yogurt3']\n    X_price = data['X_price']\n    X_featured = data['X_featured']\n    \n    # Calculate probability of buying\n    np_exponent = np.exp(beta1 * yogurt_1 + beta2 * yogurt_2 + beta3 * yogurt_3 + beta_f * X_featured + beta_p * X_price)\n    p_buy = np_exponent / (1 + np_exponent)\n    \n    # Calculate log-likelihood for each observation\n    log_likelihoods = value * np.log(p_buy) + (1 - value) * np.log(1 - p_buy)\n    \n    # Sum up log-likelihoods to get the joint log likelihood\n    joint_log_likelihood_value = np.sum(log_likelihoods)\n    \n    return -joint_log_likelihood_value  # Return negative log-likelihood for minimization\n\n# Define initial guess for parameters\ninitial_params = [1, 1, 1, 0.1, 1]\n\n# Call minimize to find MLEs\nresult = minimize(neg_log_likelihood, initial_params, args=(yogurt_melt1,))\n\n# Extract MLEs\nmle_params = result.x\n\n# Print MLEs\nprint(\"Maximum Likelihood Estimates (MLEs) for parameters:\")\nprint(\"Beta1:\", mle_params[0])\nprint(\"Beta2:\", mle_params[1])\nprint(\"Beta3:\", mle_params[2])\nprint(\"Beta_f:\", mle_params[3])\nprint(\"Beta_p:\", mle_params[4])\n\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n/opt/homebrew/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/opt/homebrew/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:590: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x) - f0\n\n\nMaximum Likelihood Estimates (MLEs) for parameters:\nBeta1: -0.5908343967104541\nBeta2: -0.33657558623289624\nBeta3: -3.440071200219531\nBeta_f: 0.13343693743472626\nBeta_p: -0.8669708026477474\n\n\n\n\nDiscussion\nWe learn…\ntodo: interpret the 3 product intercepts (which yogurt is most preferred?).\nThe intercepts for the yogurt products are the coefficients for yogurt1, yogurt2, and yogurt3. These coefficients indicate the prefrence for each yogurt product when all other variables are held constant.\nThe intercepts for the yogurt products are the coefficients for yogurt_1, yogurt_2, and yogurt_3. These coefficients indicate the preference for each yogurt product when all other variables are held constant.\nSince the coefficients are negative, it suggests that consumers are less likely to buy the respective yogurt compared to the reference category (which is likely yogurt_4, assuming it's not explicitly included in the model).\nThe magnitude of the coefficient indicates the strength of preference relative to the reference category. So, the larger the negative coefficient, the less preferred the yogurt.\nFeatured (Promotional Effect):\nThe coefficient for X_featured is -3.24661941. This indicates the effect of featuring a product on the likelihood of purchase compared to not featuring it.\nThe negative coefficient suggests that featuring a product significantly decreases the likelihood of purchase. This could be counterintuitive and might need further investigation.\nPrice Sensitivity:\nThe coefficient for X_price is -2.4392646. This represents the sensitivity of consumers to price changes.\nThe negative coefficient indicates that as the price increases, the likelihood of purchase decreases. This is consistent with economic theory and consumer behavior.\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\n\nbeta_p = -0.8669  # Estimated price coefficient\nintercepts = [-0.59083, -0.33657, -3.44006]  # Intercepts for yogurt products\n\n# Identify the most preferred and least preferred yogurt\nmost_preferred_index = np.argmax(intercepts)\nleast_preferred_index = np.argmin(intercepts)\n\n# Calculate the utility difference\nutility_difference = intercepts[most_preferred_index] - intercepts[least_preferred_index]\n\n# Calculate the dollar benefit using the price coefficient\ndollar_benefit = utility_difference * beta_p\n\nprint(\"Dollar benefit between the most-preferred and least-preferred yogurt: $\", dollar_benefit)\n\nDollar benefit between the most-preferred and least-preferred yogurt: $ -2.690415481\n\n\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\ntodo: calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities. Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?\n\nimport numpy as np\n\n# Assuming you have the fitted model parameters and the original dataset\n\n# Fitted model parameters (including intercepts)\n# beta1, beta2, beta3, beta_f, beta_p = [intercept, beta1, beta2, beta3, beta_f, beta_p]\nfitted_params = [-0.65450389, -0.40032374, -3.50331314, -3.24661941, -2.4392646]\n\n# Original price of yogurt 1\noriginal_price_yogurt1 = 0.108\n\n# Increase in price of yogurt 1\nprice_increase = 0.10\n\n# New price of yogurt 1\nnew_price_yogurt1 = original_price_yogurt1 + price_increase\n\n# Calculate the utility for each product with the new price of yogurt 1\nutility_yogurt1_new_price = fitted_params[0] + fitted_params[1] + new_price_yogurt1 * fitted_params[4]\nutility_yogurt2 = fitted_params[0] + fitted_params[2] + original_price_yogurt1 * fitted_params[4]\nutility_yogurt3 = fitted_params[0] + fitted_params[3] + original_price_yogurt1 * fitted_params[4]\nutility_yogurt4 = fitted_params[0] + fitted_params[4]\n\n# Calculate choice probabilities\nprob_yogurt1_new_price = np.exp(utility_yogurt1_new_price) / (1 + np.exp(utility_yogurt1_new_price))\nprob_yogurt2 = np.exp(utility_yogurt2) / (1 + np.exp(utility_yogurt2))\nprob_yogurt3 = np.exp(utility_yogurt3) / (1 + np.exp(utility_yogurt3))\nprob_yogurt4 = np.exp(utility_yogurt4) / (1 + np.exp(utility_yogurt4))\n\n# Calculate market shares\nmarket_share_yogurt1_new_price = prob_yogurt1_new_price\nmarket_share_yogurt2 = prob_yogurt2\nmarket_share_yogurt3 = prob_yogurt3\nmarket_share_yogurt4 = prob_yogurt4\n\n# Print market shares\nprint(\"Market share for yogurt 1 with new price: \", market_share_yogurt1_new_price)\nprint(\"Market share for yogurt 2: \", market_share_yogurt2)\nprint(\"Market share for yogurt 3: \", market_share_yogurt3)\nprint(\"Market share for yogurt 4: \", market_share_yogurt4)\n\nMarket share for yogurt 1 with new price:  0.17333195214506825\nMarket share for yogurt 2:  0.011876364167677652\nMarket share for yogurt 3:  0.015298799610551394\nMarket share for yogurt 4:  0.04336503170891939"
  },
  {
    "objectID": "projects/project3/index.html#estimating-minivan-preferences-via-conjoint-analysis",
    "href": "projects/project3/index.html#estimating-minivan-preferences-via-conjoint-analysis",
    "title": "Multi-Nomial Logit (MNL) and Conjoint Analysis",
    "section": "2. Estimating Minivan Preferences via Conjoint Analysis",
    "text": "2. Estimating Minivan Preferences via Conjoint Analysis\nConjoint Analysisis an advanced quantitative markeitng research method popular for product and pricing research. Conjoint analysis has gained popularity in recent years because the survey questions mimic the tradeoffs people make in the real world. It enables researchers to quantify how a product attribute changes demand and evaluate the market acceptance of products before they launch.\nIn this case, we will be evaluating minivan preferences via conjoint analysis.\n\nData\n\nimport numpy as np \nimport pandas as pd \nimport statsmodels.api as sm\nimport pyrsm as rsm\n\nconjoint = pd.read_csv('data/conjoint.csv')\n\nconjoint.head(6)\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n5\n1\n2\n3\nyes\n6\n2ft\nelec\n35\n0\n\n\n\n\n\n\n\n\nThis data contains the survey results from 200 respondents who compeleted 15 choice tasks with 3 alternatives given for each choice task. For each choice task there were 4 attributes seat number, cargo space, engine type, and price. Within the attributes, there are various levels. For number of seats, the levels are 6,7,8. Cargo Space has levels of 2ft and 3ft, Engine Type has levels of gas, hybrid, electric, and price in thousands of dollars). For each alternative given within the choice task, there is a binary column indicating which alternative-level combination was chosen.\nFor example, in the rows printed above we see that for the first choice task, respondent with the resp.id of 1 was given 3 choice options.\n\n\nOption 1: 6 Seats, 2ft Cargo, Gas Engine, 35K\n\n\nOption 2: 8 Seats, 3ft Cargo, Hybrid Engine, 30K\n\n\nOption 3: 6 Seats, 3ft Cargo, Gas Engine, 30K\n\n\nIn this first choice task, respondent 1 chose option 3 as indicated by the binary column choice.\n\n\nModel\n\n#converting categorical variables into numerical variables to run the model\nconjoint_dummies1 = pd.get_dummies(conjoint['cargo'], prefix = 'cargo')\nconjoint_dummies2 = pd.get_dummies(conjoint[\"eng\"], prefix = 'eng')\n\n#concatenate the dummy variables back with the original dataframe\nconcat_conjoint = pd.concat([conjoint, conjoint_dummies1, conjoint_dummies2], axis = 1)\n\n\n#converting the yes, no's for carpool into 1s and 0s\nconcat_conjoint[\"carpool\"] = rsm.ifelse(concat_conjoint[\"carpool\"] == 'yes',1,0)\nconcat_conjoint.head()\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\ncargo_2ft\ncargo_3ft\neng_elec\neng_gas\neng_hyb\n\n\n\n\n0\n1\n1\n1\n1\n6\n2ft\ngas\n35\n0\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n1\n1\n2\n1\n8\n3ft\nhyb\n30\n0\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n3\n1\n6\n3ft\ngas\n30\n1\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n1\n2\n1\n1\n6\n2ft\ngas\n30\n0\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n4\n1\n2\n2\n1\n7\n3ft\ngas\n35\n1\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n\n#converting the dummy true false variables into 1s and 0s\nconcat_conjoint[[\"cargo_2ft\", \"cargo_3ft\", \"eng_elec\", \"eng_gas\", \"eng_hyb\"]] = concat_conjoint[[\"cargo_2ft\", \"cargo_3ft\", \"eng_elec\", \"eng_gas\", \"eng_hyb\"]].astype(int)\n\n\n#omit variables to avoid multi-collinearity\nconjoint_omit = concat_conjoint[(concat_conjoint['seat'] != 6)]\n\n#design matrix\nX = conjoint_omit[['seat', 'cargo_3ft', 'eng_elec','eng_hyb', 'price']]\nX = sm.add_constant(X)  # Add intercept\n\n# Fit MNL model\nmodel = sm.MNLogit(conjoint_omit['choice'], X)\nresult = model.fit()\n\n# Display coefficients and standard errors\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.545769\n         Iterations 6\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 5976\nModel:                        MNLogit   Df Residuals:                     5970\nMethod:                           MLE   Df Model:                            5\nDate:                Thu, 16 May 2024   Pseudo R-squ.:                  0.1153\nTime:                        00:04:48   Log-Likelihood:                -3261.5\nconverged:                       True   LL-Null:                       -3686.4\nCovariance Type:            nonrobust   LLR p-value:                2.002e-181\n==============================================================================\n  choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.2928      0.525      6.276      0.000       2.264       4.321\nseat           0.2307      0.060      3.814      0.000       0.112       0.349\ncargo_3ft      0.3890      0.061      6.417      0.000       0.270       0.508\neng_elec      -1.4520      0.078    -18.702      0.000      -1.604      -1.300\neng_hyb       -0.7315      0.070    -10.424      0.000      -0.869      -0.594\nprice         -0.1556      0.008    -20.105      0.000      -0.171      -0.140\n==============================================================================\n\n\n\n\nResults\nAbove is the output from our multi-nomial logit model generated using our conjoint survey data. We can use the coefficients from the output to determine which preferences are preferred by survey respondents and quantify the change in utility they offer the respondents. The first thing to note is our constant value. In this model, our constant refers to the variables we excluded to avoid multicollinearity and they act as our baseline. For this case, this is a car that seats 6 people, has a gas engine and has a cargo space of 2ft. When analyzing the coefficient values, we will compare how changing the levels of the attributes changes utility value in comparison to the baseline value.\nIn summary, our results show us: - A one-unit increase in seat is associated with an increase in the log-odds of choosing choice=1 by approximately 0.2307, holding all other variables constant. - Similarly, having cargo_3ft is associated with an increase in the log-odds of choosing choice=1 by approximately 0.3890 compared to having cargo_2ft, holding all other variables constant. - eng_elec and eng_hyb are the indicators for electric and hybrid engines, respectively, compared to gas engines. Having an electric engine is associated with a decrease in the log-odds of choosing choice=1 by approximately 1.4520, while having a hybrid engine is associated with a decrease by approximately 0.7315, compared to gas engines. - Finally, an increase in price is associated with a decrease in the log-odds of choosing choice=1 by approximately 0.1556, holding all other variables constant.\nBased on the coefficients, people seem to prefer seat = 8, engine = gas, price = 30K, and cargo = 3ft.\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\nOur price coefficient is -0.1556, which means that a one unit increase in the price variable is associateed with a decrease in the log-odds of choicing that combination by 0.1556, holding all other variables constants.\n\n#price coefficient extracted\nprice_coeff = -0.1556\n\n# converting the price coefficient\ndollar_per_util = 1 / price_coeff\n\n# Calculate the dollar value of 3ft of cargo space compared to 2ft\ncargo_3ft_value = 0.3890  # coefficient for cargo_3ft\ncargo_2ft_value = 0  # reference category\n\ncargo_space_value_difference = (cargo_3ft_value - cargo_2ft_value) * dollar_per_util\n\nprint(f\"The dollar value of 3ft of cargo space compared to 2ft of cargo space is approximately: ${cargo_space_value_difference:.2f}\")\n\nThe dollar value of 3ft of cargo space compared to 2ft of cargo space is approximately: $-2.50\n\n\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nhint: this example is taken from the “R 4 Marketing Research” book by Chapman and Feit. I believe the same example is present in the companion book titled “Python 4 Marketing Research”. I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to “the answers,” you may consult the Chapman and Feit books."
  }
]